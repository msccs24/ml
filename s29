import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

iris = load_iris()
X = iris.data          
y = iris.target
t_names = iris.target_names
f_names = iris.feature_names

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_test)

pca = PCA()

X_train_pca = pca.fit_transform(X_train_std)
X_test_pca = pca.transform(X_test_std)

print(f"\nPCA explained_variance_ratio : {pca.explained_variance_ratio_}")

cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)
print(f"\nPCA cumulative_variance ratio :",cumulative_variance_ratio)

#var = 0.99
#var = 0.95
var = 0.90
#var = 0.80
#var = 0.50
n_c = np.argmax(cumulative_variance_ratio >= var) + 1

pca = PCA(n_components = n_c)
X_train_pca = pca.fit_transform(X_train_std)
X_test_pca = pca.transform(X_test_std)

print("\n Original Training Data Shape : ",X_train.shape)
print("\n Reduced Dimentiality after applying PCA : ",X_train_pca.shape)
print(f"\n Total explained variance by selected components : {np.sum(pca.explained_variance_ratio_):.2f}")

L_model = LogisticRegression(random_state = 0)
L_model.fit(X_train_pca, y_train)
y_pred = L_model.predict(X_test_pca)
print(f"\n Model accuracy on test set : {accuracy_score(y_test, y_pred):.2f}")

new_flower1_measurements = np.array([[5.1,3.5,1.4,0.2]])
new_flower1_scaled = scaler.transform(new_flower1_measurements)
new_flower1_pca = pca.transform(new_flower1_scaled)

prediction = L_model.predict(new_flower1_pca)
predicted_species = t_names[prediction[0]]
print(f"\n New flower 1 measurements : {new_flower1_measurements}")
print(f"\n Prediction species : {predicted_species}")

new_flower2_measurements = np.array([[6.2, 2.2, 4.5, 1.5]])
new_flower2_scaled = scaler.transform(new_flower2_measurements)
new_flower2_pca = pca.transform(new_flower2_scaled)

prediction = L_model.predict(new_flower2_pca)
predicted_species = t_names[prediction[0]]
print(f"\n New flower 2 measurements : {new_flower2_measurements}")
print(f"\n Prediction species : {predicted_species}")


Q2
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv(r"ML\Employee_Salary_Dataset.csv")
print(df.head(5))
print("Information :\n")
df.info()
print("describe :\n",df.describe())
df.dropna(inplace=True)

X = df[['Salary']]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("\n1.Elbow method :")
wcss = []
K_e = range(1,11)
for k in K_e:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)
    print(f"k={k}, WCSS={kmeans.inertia_:.2f}")

print(f"\nFor k = {k} WCSS : {wcss}")
plt.figure(figsize=(10,5))
plt.plot(K_e, wcss, marker='*')
plt.title(f'Elbow Method for Optimal k={k}')
plt.xlabel('Number of clusters (k)')
plt.ylabel('ECSS Inertia')
plt.xticks(K_e)
plt.grid(True)
plt.legend()
plt.show()

print("\n2.Silhouette Score :")
sil_score = []
K_s = range(2,11)
for k in K_s:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    score = silhouette_score(X_scaled, kmeans.labels_)
    sil_score.append(score)

print(f"For k = {k}, Silhouette Score = {score:.3f}")

plt.figure(figsize=(10,5))
plt.plot(K_s, sil_score, marker='*')
plt.title(f'Silhouette Score for Optimal k={k}')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.xticks(K_s)
plt.grid(True)
plt.legend()
plt.show()

optimal_k = 3 
print(f"\nFinal K-Means Model (k={optimal_k})")
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df['Income_Group'] = kmeans.fit_predict(X_scaled)


centers = scaler.inverse_transform(kmeans.cluster_centers_)
centroid_labels = [f"Group {i+1}: {int(c[0])}  (avg income)" for i, c in enumerate(centers)]
print("\nCluster Centers (approx):\n", centroid_labels)

print("\n Employee data with assigned income group")
print(df.head())

plt.figure(figsize=(8,5))
sns.scatterplot(x=df.index, y='Salary', hue='Income_Group', palette='tab10', data=df)
plt.title(f'Employee Income Classified into {optimal_k} Groups')
plt.xlabel('Employee Index')
plt.ylabel('Annual Salary')
plt.legend()
plt.show()



